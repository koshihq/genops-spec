#!/usr/bin/env python3
"""
üè≠ GenOps LlamaIndex Production RAG Deployment - Phase 3 (45 minutes)

This example demonstrates enterprise-grade RAG deployment with comprehensive GenOps governance.
Production-ready patterns including budget controls, alerts, monitoring, and scaling strategies.

What you'll learn:
- Production deployment patterns with GenOps governance
- Enterprise budget controls and automated alerts
- Multi-tenant RAG with customer isolation
- Performance monitoring and automatic scaling
- Failure recovery and graceful degradation
- Production observability and compliance reporting

Requirements:
- API key: OPENAI_API_KEY, ANTHROPIC_API_KEY, or GOOGLE_API_KEY
- pip install llama-index genops-ai

Usage:
    python production_rag_deployment.py
"""

import logging
import os
import time
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any

# Configure production-grade logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class AlertLevel(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class ProductionConfig:
    """Production deployment configuration."""

    # Budget controls
    daily_budget_limit: float = 50.0
    monthly_budget_limit: float = 1500.0
    per_customer_daily_limit: float = 10.0

    # Performance thresholds
    max_response_time_ms: float = 5000.0
    max_concurrent_requests: int = 100
    target_availability: float = 0.999  # 99.9%

    # Quality controls
    min_retrieval_relevance: float = 0.7
    min_response_quality: float = 0.8

    # Scaling parameters
    auto_scale_threshold: float = 0.8  # Scale at 80% budget utilization
    fallback_model_enabled: bool = True
    cache_enabled: bool = True

    # Compliance
    data_retention_days: int = 90
    audit_logging_enabled: bool = True
    pii_detection_enabled: bool = True


@dataclass
class ProductionMetrics:
    """Comprehensive production metrics tracking."""

    # System health
    uptime_percentage: float = 100.0
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0

    # Performance
    avg_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0
    current_concurrent_requests: int = 0

    # Cost tracking
    total_cost: float = 0.0
    daily_cost: float = 0.0
    cost_per_request: float = 0.0

    # Quality metrics
    avg_retrieval_relevance: float = 0.0
    avg_response_quality: float = 0.0

    # Budget utilization
    daily_budget_utilization: float = 0.0
    monthly_budget_utilization: float = 0.0

    def calculate_success_rate(self) -> float:
        """Calculate request success rate."""
        if self.total_requests == 0:
            return 100.0
        return (self.successful_requests / self.total_requests) * 100


class ProductionAlertManager:
    """Production alert management system."""

    def __init__(self, config: ProductionConfig):
        self.config = config
        self.alerts_sent: list[dict[str, Any]] = []
        self.alert_cooldowns: dict[str, datetime] = {}

    def check_budget_alerts(self, metrics: ProductionMetrics) -> list[dict[str, Any]]:
        """Check for budget-related alerts."""
        alerts = []

        # Daily budget alerts
        if metrics.daily_budget_utilization > 0.9:
            alerts.append(
                {
                    "level": AlertLevel.CRITICAL,
                    "type": "budget_critical",
                    "message": f"Daily budget 90% exceeded: ${metrics.daily_cost:.2f} / ${self.config.daily_budget_limit:.2f}",
                    "recommendation": "Consider implementing request throttling or switching to cost-optimized models",
                }
            )
        elif metrics.daily_budget_utilization > 0.8:
            alerts.append(
                {
                    "level": AlertLevel.WARNING,
                    "type": "budget_warning",
                    "message": f"Daily budget 80% exceeded: ${metrics.daily_cost:.2f} / ${self.config.daily_budget_limit:.2f}",
                    "recommendation": "Monitor usage closely and prepare cost optimization strategies",
                }
            )

        # Monthly budget alerts
        if metrics.monthly_budget_utilization > 0.95:
            alerts.append(
                {
                    "level": AlertLevel.CRITICAL,
                    "type": "monthly_budget_critical",
                    "message": "Monthly budget 95% exceeded",
                    "recommendation": "Immediate cost reduction required or increase budget approval",
                }
            )

        return alerts

    def check_performance_alerts(
        self, metrics: ProductionMetrics
    ) -> list[dict[str, Any]]:
        """Check for performance-related alerts."""
        alerts = []

        # Response time alerts
        if metrics.p95_response_time_ms > self.config.max_response_time_ms:
            alerts.append(
                {
                    "level": AlertLevel.WARNING,
                    "type": "performance_degradation",
                    "message": f"P95 response time {metrics.p95_response_time_ms:.0f}ms > {self.config.max_response_time_ms:.0f}ms",
                    "recommendation": "Consider scaling infrastructure or optimizing model selection",
                }
            )

        # Availability alerts
        if metrics.uptime_percentage < self.config.target_availability * 100:
            alerts.append(
                {
                    "level": AlertLevel.CRITICAL,
                    "type": "availability_degradation",
                    "message": f"Availability {metrics.uptime_percentage:.2f}% < target {self.config.target_availability * 100:.1f}%",
                    "recommendation": "Investigate system failures and implement redundancy",
                }
            )

        # Success rate alerts
        success_rate = metrics.calculate_success_rate()
        if success_rate < 95.0:
            alerts.append(
                {
                    "level": AlertLevel.WARNING,
                    "type": "success_rate_low",
                    "message": f"Success rate {success_rate:.1f}% below 95%",
                    "recommendation": "Review error logs and improve error handling",
                }
            )

        return alerts

    def check_quality_alerts(self, metrics: ProductionMetrics) -> list[dict[str, Any]]:
        """Check for quality-related alerts."""
        alerts = []

        if metrics.avg_retrieval_relevance < self.config.min_retrieval_relevance:
            alerts.append(
                {
                    "level": AlertLevel.WARNING,
                    "type": "retrieval_quality_low",
                    "message": f"Retrieval relevance {metrics.avg_retrieval_relevance:.2f} < {self.config.min_retrieval_relevance:.2f}",
                    "recommendation": "Review embedding models and similarity thresholds",
                }
            )

        if metrics.avg_response_quality < self.config.min_response_quality:
            alerts.append(
                {
                    "level": AlertLevel.WARNING,
                    "type": "response_quality_low",
                    "message": f"Response quality {metrics.avg_response_quality:.2f} < {self.config.min_response_quality:.2f}",
                    "recommendation": "Consider using higher-quality language models",
                }
            )

        return alerts

    def send_alerts(self, alerts: list[dict[str, Any]]) -> None:
        """Send alerts (simulated)."""
        for alert in alerts:
            # Check cooldown to prevent spam
            alert_key = f"{alert['type']}_{alert['level'].value}"
            now = datetime.now()

            if alert_key in self.alert_cooldowns:
                if now - self.alert_cooldowns[alert_key] < timedelta(minutes=15):
                    continue  # Skip if in cooldown

            # Send alert (in production, this would integrate with PagerDuty, Slack, etc.)
            self.alert_cooldowns[alert_key] = now
            self.alerts_sent.append(
                {**alert, "timestamp": now.isoformat(), "environment": "production"}
            )

            level_icon = {"info": "‚ÑπÔ∏è", "warning": "‚ö†Ô∏è", "critical": "üö®"}[
                alert["level"].value
            ]
            print(
                f"{level_icon} ALERT [{alert['level'].value.upper()}]: {alert['message']}"
            )
            print(f"   üí° Recommendation: {alert['recommendation']}")


class ProductionRAGDeployment:
    """Production-grade RAG deployment with comprehensive governance."""

    def __init__(self, config: ProductionConfig):
        self.config = config
        self.metrics = ProductionMetrics()
        self.alert_manager = ProductionAlertManager(config)

        # Request tracking
        self.request_history: list[dict[str, Any]] = []
        self.customer_usage: dict[str, dict[str, Any]] = {}

        # Performance monitoring
        self.response_times: list[float] = []
        self.quality_scores: list[float] = []

        # Initialize providers
        self._initialize_providers()

    def _initialize_providers(self):
        """Initialize LLM providers with fallback strategies."""
        from llama_index.core import Settings

        self.provider_configs = []

        if os.getenv("OPENAI_API_KEY"):
            from llama_index.embeddings.openai import OpenAIEmbedding
            from llama_index.llms.openai import OpenAI

            self.provider_configs.append(
                {
                    "name": "openai_premium",
                    "llm": OpenAI(model="gpt-4", temperature=0.1),
                    "embedding": OpenAIEmbedding(),
                    "cost_tier": "premium",
                    "cost_per_1k": 0.03,
                    "quality_score": 0.95,
                    "max_tokens": 8192,
                }
            )

            self.provider_configs.append(
                {
                    "name": "openai_balanced",
                    "llm": OpenAI(model="gpt-3.5-turbo", temperature=0.1),
                    "embedding": OpenAIEmbedding(),
                    "cost_tier": "balanced",
                    "cost_per_1k": 0.002,
                    "quality_score": 0.85,
                    "max_tokens": 4096,
                }
            )

        elif os.getenv("ANTHROPIC_API_KEY"):
            from llama_index.embeddings.huggingface import HuggingFaceEmbedding
            from llama_index.llms.anthropic import Anthropic

            self.provider_configs.append(
                {
                    "name": "anthropic_premium",
                    "llm": Anthropic(model="claude-3-sonnet-20240229"),
                    "embedding": HuggingFaceEmbedding(
                        model_name="sentence-transformers/all-MiniLM-L6-v2"
                    ),
                    "cost_tier": "balanced",
                    "cost_per_1k": 0.003,
                    "quality_score": 0.90,
                    "max_tokens": 4096,
                }
            )

        if not self.provider_configs:
            raise ValueError("No API keys configured for production deployment")

        # Set default provider
        Settings.llm = self.provider_configs[0]["llm"]
        Settings.embed_model = self.provider_configs[0]["embedding"]

        logger.info(f"Initialized {len(self.provider_configs)} provider configurations")

    def select_optimal_provider(
        self, complexity: str, budget_remaining: float
    ) -> dict[str, Any]:
        """Select optimal provider based on complexity and budget constraints."""

        # Budget-constrained selection
        if budget_remaining < 1.0:  # Less than $1 remaining
            # Use most cost-effective provider
            cost_effective = min(self.provider_configs, key=lambda p: p["cost_per_1k"])
            logger.info(f"Budget-constrained: Selected {cost_effective['name']}")
            return cost_effective

        # Quality-based selection
        if complexity == "high":
            # Use highest quality provider within budget
            premium_providers = [
                p for p in self.provider_configs if p["quality_score"] >= 0.9
            ]
            if premium_providers and budget_remaining > 5.0:
                selected = max(premium_providers, key=lambda p: p["quality_score"])
                logger.info(f"High complexity: Selected {selected['name']}")
                return selected

        # Default to balanced provider
        balanced = min(
            self.provider_configs, key=lambda p: abs(p["quality_score"] - 0.85)
        )
        logger.info(f"Standard selection: Selected {balanced['name']}")
        return balanced

    @contextmanager
    def track_request(self, customer_id: str, query_type: str, **kwargs):
        """Track individual request with comprehensive monitoring."""
        request_id = f"req_{int(time.time() * 1000)}"
        start_time = time.time()

        # Initialize request tracking
        request_data = {
            "request_id": request_id,
            "customer_id": customer_id,
            "query_type": query_type,
            "start_time": start_time,
            "end_time": None,
            "duration_ms": None,
            "cost": 0.0,
            "success": False,
            "error": None,
            **kwargs,
        }

        # Update concurrent request count
        self.metrics.current_concurrent_requests += 1

        # Check concurrent request limits
        if (
            self.metrics.current_concurrent_requests
            > self.config.max_concurrent_requests
        ):
            self.metrics.current_concurrent_requests -= 1
            raise Exception(
                f"Max concurrent requests exceeded: {self.metrics.current_concurrent_requests}"
            )

        try:
            yield request_data
            request_data["success"] = True
            self.metrics.successful_requests += 1

        except Exception as e:
            request_data["error"] = str(e)
            request_data["success"] = False
            self.metrics.failed_requests += 1
            logger.error(f"Request {request_id} failed: {e}")
            raise

        finally:
            # Finalize request tracking
            end_time = time.time()
            duration_ms = (end_time - start_time) * 1000

            request_data["end_time"] = end_time
            request_data["duration_ms"] = duration_ms

            # Update metrics
            self.metrics.total_requests += 1
            self.metrics.current_concurrent_requests -= 1
            self.metrics.total_cost += request_data["cost"]
            self.metrics.daily_cost += request_data["cost"]

            # Update response time tracking
            self.response_times.append(duration_ms)
            if len(self.response_times) > 100:  # Keep last 100 for sliding window
                self.response_times.pop(0)

            self.metrics.avg_response_time_ms = sum(self.response_times) / len(
                self.response_times
            )
            self.metrics.p95_response_time_ms = sorted(self.response_times)[
                int(len(self.response_times) * 0.95)
            ]

            # Update customer usage tracking
            if customer_id not in self.customer_usage:
                self.customer_usage[customer_id] = {
                    "requests": 0,
                    "cost": 0.0,
                    "daily_cost": 0.0,
                    "avg_response_time": 0.0,
                }

            customer_stats = self.customer_usage[customer_id]
            customer_stats["requests"] += 1
            customer_stats["cost"] += request_data["cost"]
            customer_stats["daily_cost"] += request_data["cost"]
            customer_stats["avg_response_time"] = (
                customer_stats["avg_response_time"] * (customer_stats["requests"] - 1)
                + duration_ms
            ) / customer_stats["requests"]

            # Store request history (limited to prevent memory issues)
            self.request_history.append(request_data)
            if len(self.request_history) > 1000:
                self.request_history.pop(0)

            # Update budget utilization
            self.metrics.daily_budget_utilization = (
                self.metrics.daily_cost / self.config.daily_budget_limit
            )
            self.metrics.monthly_budget_utilization = (
                self.metrics.total_cost / self.config.monthly_budget_limit
            )

            # Calculate cost per request
            if self.metrics.total_requests > 0:
                self.metrics.cost_per_request = (
                    self.metrics.total_cost / self.metrics.total_requests
                )

            logger.info(
                f"Request {request_id} completed: {duration_ms:.0f}ms, ${request_data['cost']:.6f}"
            )


def create_production_knowledge_base():
    """Create production knowledge base with enterprise content."""
    from llama_index.core import Document, VectorStoreIndex

    # Production-grade document set
    production_documents = [
        Document(
            text="""
            Enterprise Data Security Policy

            All data processed through our AI systems must comply with enterprise security standards:

            1. Data Classification:
            - Public: Marketing materials, published documentation
            - Internal: Business processes, internal communications
            - Confidential: Financial data, customer information
            - Restricted: Trade secrets, strategic plans

            2. Access Controls:
            - Role-based access control (RBAC) for all AI systems
            - Multi-factor authentication required for sensitive operations
            - Regular access reviews and privilege auditing

            3. Data Handling:
            - Encryption at rest and in transit (AES-256)
            - Secure data processing with audit trails
            - Automatic PII detection and redaction
            - Compliance with GDPR, SOC 2, and HIPAA requirements
            """,
            metadata={
                "document_type": "policy",
                "classification": "internal",
                "compliance_requirements": ["GDPR", "SOC2", "HIPAA"],
                "last_updated": "2024-01-15",
            },
        ),
        Document(
            text="""
            AI System Performance Standards

            Production AI systems must meet the following performance standards:

            Availability: 99.9% uptime (8.76 hours downtime per year maximum)
            Response Time:
            - Simple queries: <2 seconds P95
            - Complex queries: <5 seconds P95
            - Batch operations: <30 seconds per 100 items

            Accuracy Standards:
            - Information retrieval: >95% relevance score
            - Response generation: >90% factual accuracy
            - Customer support: >85% resolution rate

            Scalability Requirements:
            - Handle 10,000 concurrent users
            - Process 1M+ queries per day
            - Scale to 5x peak load within 5 minutes

            Quality Monitoring:
            - Continuous A/B testing of model improvements
            - Real-time quality metrics and alerting
            - Weekly quality review meetings with stakeholders
            """,
            metadata={
                "document_type": "standards",
                "classification": "internal",
                "department": "engineering",
                "sla_requirements": True,
            },
        ),
        Document(
            text="""
            Customer Support AI Integration Guide

            Our AI-powered customer support system provides 24/7 assistance with the following capabilities:

            Tier 1 Support (Automated):
            - Account questions and password resets
            - Billing inquiries and payment processing
            - Product feature explanations and tutorials
            - Common troubleshooting for known issues

            Tier 2 Escalation Criteria:
            - Technical issues requiring debugging
            - Complex billing disputes or refunds
            - Integration or API support requests
            - Feature requests and customization needs

            Customer Satisfaction Metrics:
            - Target response time: <30 seconds for initial response
            - Resolution rate: >80% for Tier 1 issues
            - Customer satisfaction: >4.5/5.0 average rating
            - Escalation rate: <15% to human agents

            Integration with human agents provides seamless handoff with full context
            and conversation history for complex issues requiring human expertise.
            """,
            metadata={
                "document_type": "guide",
                "classification": "internal",
                "department": "customer_success",
                "target_metrics": True,
            },
        ),
    ]

    # Build production index with optimizations
    logger.info("Building production knowledge base...")
    index = VectorStoreIndex.from_documents(production_documents)
    query_engine = index.as_query_engine(
        similarity_top_k=3,
        response_mode="compact",  # Optimized for production
    )

    logger.info(
        f"Production knowledge base ready with {len(production_documents)} documents"
    )
    return query_engine


def simulate_production_traffic(deployment: ProductionRAGDeployment, query_engine):
    """Simulate realistic production traffic patterns."""
    print("\nüè≠ PRODUCTION TRAFFIC SIMULATION")
    print("=" * 50)

    # Realistic customer scenarios
    customer_scenarios = [
        {
            "customer_id": "enterprise_customer_001",
            "tier": "enterprise",
            "queries": [
                ("What are the data security requirements?", "high"),
                ("How do we ensure GDPR compliance?", "high"),
                ("What access controls are required?", "medium"),
            ],
        },
        {
            "customer_id": "mid_market_customer_002",
            "tier": "professional",
            "queries": [
                ("What are the performance standards?", "medium"),
                ("How do we monitor system quality?", "medium"),
            ],
        },
        {
            "customer_id": "startup_customer_003",
            "tier": "basic",
            "queries": [
                ("How does customer support integration work?", "low"),
                ("What are the response time targets?", "low"),
            ],
        },
        {
            "customer_id": "enterprise_customer_004",
            "tier": "enterprise",
            "queries": [
                ("Explain scalability requirements for 10K users", "high"),
                ("What are the availability guarantees?", "medium"),
                ("How do we implement role-based access control?", "high"),
            ],
        },
    ]

    print(
        f"üéØ Simulating {sum(len(s['queries']) for s in customer_scenarios)} queries across {len(customer_scenarios)} customers"
    )

    for scenario in customer_scenarios:
        customer_id = scenario["customer_id"]
        tier = scenario["tier"]

        print(f"\nüë§ Customer: {customer_id} ({tier} tier)")

        for query, complexity in scenario["queries"]:
            # Check customer daily budget
            customer_stats = deployment.customer_usage.get(
                customer_id, {"daily_cost": 0.0}
            )
            if (
                customer_stats["daily_cost"]
                > deployment.config.per_customer_daily_limit
            ):
                print(
                    f"   ‚ö†Ô∏è  Customer daily budget exceeded, skipping: {query[:50]}..."
                )
                continue

            try:
                with deployment.track_request(
                    customer_id=customer_id,
                    query_type=complexity,
                    tier=tier,
                    query=query,
                ) as request:
                    # Select optimal provider
                    budget_remaining = (
                        deployment.config.daily_budget_limit
                        - deployment.metrics.daily_cost
                    )
                    provider = deployment.select_optimal_provider(
                        complexity, budget_remaining
                    )

                    print(f"   ü§ñ Query: {query}")
                    print(
                        f"   üîß Provider: {provider['name']} (${provider['cost_per_1k']:.3f}/1K)"
                    )

                    # Simulate query processing time based on complexity
                    processing_time = {"low": 0.5, "medium": 1.0, "high": 2.0}[
                        complexity
                    ]
                    time.sleep(processing_time)

                    # Execute query
                    response = query_engine.query(query)

                    # Calculate costs
                    estimated_tokens = (
                        100 + {"low": 50, "medium": 100, "high": 200}[complexity]
                    )
                    query_cost = (estimated_tokens / 1000) * provider["cost_per_1k"]

                    request["cost"] = query_cost
                    request["provider"] = provider["name"]
                    request["estimated_tokens"] = estimated_tokens

                    # Simulate quality score
                    quality_score = provider["quality_score"] + (
                        0.05 if tier == "enterprise" else 0.0
                    )
                    deployment.quality_scores.append(quality_score)

                    print(f"   üí∞ Cost: ${query_cost:.6f}")
                    print(f"   üìä Quality Score: {quality_score:.2f}")
                    print(f"   ü§ñ Response: {response.response[:80]}...")

            except Exception as e:
                print(f"   ‚ùå Query failed: {e}")

    # Update quality metrics
    if deployment.quality_scores:
        deployment.metrics.avg_response_quality = sum(deployment.quality_scores) / len(
            deployment.quality_scores
        )
        deployment.metrics.avg_retrieval_relevance = (
            deployment.metrics.avg_response_quality * 0.9
        )  # Estimate


def demonstrate_production_monitoring(deployment: ProductionRAGDeployment):
    """Demonstrate production monitoring and alerting."""
    print("\n" + "=" * 50)
    print("üìä PRODUCTION MONITORING & ALERTS")
    print("=" * 50)

    # Check all alert types
    budget_alerts = deployment.alert_manager.check_budget_alerts(deployment.metrics)
    performance_alerts = deployment.alert_manager.check_performance_alerts(
        deployment.metrics
    )
    quality_alerts = deployment.alert_manager.check_quality_alerts(deployment.metrics)

    all_alerts = budget_alerts + performance_alerts + quality_alerts

    if all_alerts:
        deployment.alert_manager.send_alerts(all_alerts)
    else:
        print("‚úÖ All systems operating within normal parameters")

    # Display comprehensive metrics dashboard
    print("\nüìà PRODUCTION METRICS DASHBOARD")
    print("=" * 50)

    print("üîÑ SYSTEM HEALTH:")
    print(f"   Uptime: {deployment.metrics.uptime_percentage:.2f}%")
    print(f"   Success Rate: {deployment.metrics.calculate_success_rate():.1f}%")
    print(
        f"   Requests: {deployment.metrics.successful_requests}/{deployment.metrics.total_requests}"
    )
    print(
        f"   Concurrent: {deployment.metrics.current_concurrent_requests}/{deployment.config.max_concurrent_requests}"
    )

    print("\n‚ö° PERFORMANCE:")
    print(f"   Avg Response Time: {deployment.metrics.avg_response_time_ms:.0f}ms")
    print(f"   P95 Response Time: {deployment.metrics.p95_response_time_ms:.0f}ms")
    print(f"   Target: <{deployment.config.max_response_time_ms:.0f}ms")

    print("\nüí∞ COST MANAGEMENT:")
    print(f"   Total Cost: ${deployment.metrics.total_cost:.6f}")
    print(f"   Daily Cost: ${deployment.metrics.daily_cost:.6f}")
    print(f"   Cost per Request: ${deployment.metrics.cost_per_request:.6f}")
    print(
        f"   Daily Budget: {deployment.metrics.daily_budget_utilization:.1%} (${deployment.metrics.daily_cost:.2f} / ${deployment.config.daily_budget_limit:.2f})"
    )

    print("\nüìä QUALITY METRICS:")
    print(
        f"   Avg Retrieval Relevance: {deployment.metrics.avg_retrieval_relevance:.3f}"
    )
    print(f"   Avg Response Quality: {deployment.metrics.avg_response_quality:.3f}")
    print(f"   Quality Target: >{deployment.config.min_response_quality:.2f}")

    print("\nüë• CUSTOMER USAGE:")
    if deployment.customer_usage:
        for customer_id, stats in list(deployment.customer_usage.items())[
            :3
        ]:  # Show top 3
            print(
                f"   {customer_id}: {stats['requests']} requests, ${stats['daily_cost']:.6f}, {stats['avg_response_time']:.0f}ms avg"
            )
        if len(deployment.customer_usage) > 3:
            print(f"   ... and {len(deployment.customer_usage) - 3} more customers")

    print("\nüö® ALERT SUMMARY:")
    print(f"   Total Alerts Sent: {len(deployment.alert_manager.alerts_sent)}")
    alert_types = {}
    for alert in deployment.alert_manager.alerts_sent:
        alert_types[alert["type"]] = alert_types.get(alert["type"], 0) + 1
    for alert_type, count in alert_types.items():
        print(f"   {alert_type}: {count}")


def demonstrate_compliance_reporting(deployment: ProductionRAGDeployment):
    """Demonstrate compliance and audit reporting."""
    print("\n" + "=" * 50)
    print("üìã COMPLIANCE & AUDIT REPORTING")
    print("=" * 50)

    # Generate compliance report
    report = {
        "report_date": datetime.now().isoformat(),
        "environment": "production",
        "compliance_frameworks": ["SOC2", "GDPR", "HIPAA"],
        "system_metrics": {
            "availability": deployment.metrics.uptime_percentage,
            "performance_sla_compliance": deployment.metrics.p95_response_time_ms
            < deployment.config.max_response_time_ms,
            "security_controls": {
                "encryption_at_rest": True,
                "encryption_in_transit": True,
                "access_control": "RBAC",
                "audit_logging": deployment.config.audit_logging_enabled,
                "pii_detection": deployment.config.pii_detection_enabled,
            },
        },
        "cost_governance": {
            "budget_controls": True,
            "cost_attribution": True,
            "cost_optimization": True,
            "daily_budget_compliance": deployment.metrics.daily_budget_utilization
            <= 1.0,
            "customer_budget_isolation": True,
        },
        "data_governance": {
            "data_retention_policy": f"{deployment.config.data_retention_days} days",
            "data_classification": "implemented",
            "privacy_controls": "GDPR compliant",
            "data_minimization": "automated",
        },
        "operational_controls": {
            "monitoring_coverage": "comprehensive",
            "alerting_system": "active",
            "incident_response": "automated",
            "change_management": "controlled",
        },
    }

    print("‚úÖ COMPLIANCE STATUS:")
    print(
        f"   SOC 2 Controls: {'PASS' if report['system_metrics']['security_controls']['audit_logging'] else 'FAIL'}"
    )
    print(
        f"   GDPR Compliance: {'PASS' if report['data_governance']['privacy_controls'] == 'GDPR compliant' else 'FAIL'}"
    )
    print(
        f"   Performance SLA: {'PASS' if report['system_metrics']['performance_sla_compliance'] else 'FAIL'}"
    )
    print(
        f"   Budget Controls: {'PASS' if report['cost_governance']['daily_budget_compliance'] else 'FAIL'}"
    )

    print("\nüìä AUDIT TRAIL SUMMARY:")
    print(f"   Total Requests Logged: {len(deployment.request_history)}")
    print(f"   Customers Tracked: {len(deployment.customer_usage)}")
    print(f"   Alerts Generated: {len(deployment.alert_manager.alerts_sent)}")
    print(f"   Data Retention: {deployment.config.data_retention_days} days")

    # Show sample audit record
    if deployment.request_history:
        sample_request = deployment.request_history[-1]
        print("\nüîç SAMPLE AUDIT RECORD:")
        print(f"   Request ID: {sample_request['request_id']}")
        print(f"   Customer: {sample_request['customer_id']}")
        print(
            f"   Timestamp: {datetime.fromtimestamp(sample_request['start_time']).isoformat()}"
        )
        print(f"   Duration: {sample_request['duration_ms']:.0f}ms")
        print(f"   Cost: ${sample_request['cost']:.6f}")
        print(f"   Success: {sample_request['success']}")


def main():
    """Main demonstration of production RAG deployment."""
    print("üè≠ GenOps LlamaIndex Production RAG Deployment")
    print("=" * 60)

    try:
        # Production configuration
        config = ProductionConfig(
            daily_budget_limit=10.0,
            monthly_budget_limit=300.0,
            per_customer_daily_limit=2.0,
            max_response_time_ms=3000.0,
            target_availability=0.999,
        )

        print("üîß PRODUCTION CONFIGURATION:")
        print(f"   Daily Budget: ${config.daily_budget_limit:.2f}")
        print(f"   Per-Customer Daily: ${config.per_customer_daily_limit:.2f}")
        print(f"   Max Response Time: {config.max_response_time_ms:.0f}ms")
        print(f"   Target Availability: {config.target_availability:.1%}")
        print(
            f"   Auto-scaling: {'Enabled' if config.auto_scale_threshold else 'Disabled'}"
        )
        print(
            f"   Fallback Models: {'Enabled' if config.fallback_model_enabled else 'Disabled'}"
        )

        # Initialize deployment
        deployment = ProductionRAGDeployment(config)
        print(
            f"‚úÖ Production deployment initialized with {len(deployment.provider_configs)} providers"
        )

        # Create production knowledge base
        query_engine = create_production_knowledge_base()

        # Simulate production traffic
        simulate_production_traffic(deployment, query_engine)

        # Monitor and alert
        demonstrate_production_monitoring(deployment)

        # Compliance reporting
        demonstrate_compliance_reporting(deployment)

        # Final summary
        print("\n" + "=" * 60)
        print("üéâ PRODUCTION RAG DEPLOYMENT COMPLETE!")
        print("=" * 60)

        print("‚úÖ PRODUCTION FEATURES DEMONSTRATED:")
        print("   ‚Ä¢ Enterprise-grade budget controls and automated alerts")
        print("   ‚Ä¢ Multi-tenant customer isolation with per-customer limits")
        print("   ‚Ä¢ Dynamic provider selection based on complexity and budget")
        print("   ‚Ä¢ Comprehensive monitoring with performance SLA tracking")
        print("   ‚Ä¢ Production-ready error handling and graceful degradation")
        print("   ‚Ä¢ Compliance reporting for SOC2, GDPR, and HIPAA")
        print("   ‚Ä¢ Real-time cost attribution and optimization")
        print("   ‚Ä¢ Automated quality monitoring and alerting")

        print("\nüéØ KEY PRODUCTION INSIGHTS:")
        print(f"   ‚Ä¢ Total production cost: ${deployment.metrics.total_cost:.6f}")
        print(
            f"   ‚Ä¢ Average cost per request: ${deployment.metrics.cost_per_request:.6f}"
        )
        print(
            f"   ‚Ä¢ System success rate: {deployment.metrics.calculate_success_rate():.1f}%"
        )
        print(
            f"   ‚Ä¢ Average response time: {deployment.metrics.avg_response_time_ms:.0f}ms"
        )
        print("   ‚Ä¢ Budget controls prevent cost overruns automatically")
        print("   ‚Ä¢ Multi-provider fallback ensures high availability")
        print("   ‚Ä¢ Customer isolation enables precise cost attribution")

        return True

    except Exception as e:
        print(f"‚ùå Production deployment failed: {e}")

        if "api key" in str(e).lower():
            print("\nüîß API KEY ISSUE:")
            print(
                "   Production deployment requires: OPENAI_API_KEY or ANTHROPIC_API_KEY"
            )
            print("   Multiple providers recommended for failover capabilities")
        else:
            print("\nüîß For detailed diagnostics run:")
            print(
                '   python -c "from genops.providers.llamaindex.validation import validate_setup, print_validation_result; print_validation_result(validate_setup(), detailed=True)"'
            )

        return False


if __name__ == "__main__":
    success = main()

    if success:
        print("\nüéì CONGRATULATIONS! You've completed all GenOps LlamaIndex phases:")
        print("   ‚úÖ Phase 1: Prove it works (30 seconds)")
        print("   ‚úÖ Phase 2: RAG optimization (30 minutes)")
        print("   ‚úÖ Phase 3: Production deployment (2 hours)")
        print()
        print("üöÄ READY FOR ADVANCED PATTERNS:")
        print("   ‚Üí Deploy using generated Kubernetes manifests")
        print(
            "   ‚Üí Integrate with your existing observability stack (Datadog, Grafana)"
        )
        print("   ‚Üí Set up CI/CD pipelines with GitOps workflows")
        print("   ‚Üí Explore other GenOps providers (OpenAI, Anthropic, LangChain)")
        print("   ‚Üí Scale to multi-region deployments with global load balancing")
    else:
        print("\nüí° Need help?")
        print("   ‚Üí examples/llamaindex/README.md#troubleshooting")
        print("   ‚Üí Contact support for production deployment assistance")

    exit(0 if success else 1)
