#!/usr/bin/env python3
"""
OpenRouter Auto-Instrumentation Example

Demonstrates zero-code auto-instrumentation with OpenRouter.
Shows how existing OpenRouter code gets automatic governance telemetry.

Usage:
    export OPENROUTER_API_KEY="your-key"
    python auto_instrumentation.py

Key features demonstrated:
- Zero-code auto-instrumentation setup
- Existing OpenRouter code works unchanged
- Automatic governance telemetry capture
- Global default governance attributes
"""

import os


def demonstrate_auto_instrumentation():
    """Show how auto-instrumentation works with existing OpenRouter code."""

    print("ğŸ¯ OpenRouter Auto-Instrumentation Demo")
    print("=" * 50)

    # Check for API key
    api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ Missing API key. Set OPENROUTER_API_KEY environment variable.")
        return

    try:
        print("ğŸ”§ Step 1: Initialize GenOps auto-instrumentation")
        print("   Code: genops.init()")

        # Initialize GenOps auto-instrumentation - this is the ONLY change needed
        import genops

        genops.init(
            service_name="openrouter-demo",
            default_team="ai-platform-team",
            default_project="multi-provider-experiment",
            default_environment="development",
        )
        print("   âœ… Auto-instrumentation enabled!")

        print("\nğŸ“± Step 2: Use existing OpenRouter code (unchanged!)")
        print("   Code: Standard OpenAI SDK with OpenRouter base URL")

        # This is standard OpenRouter code - no changes needed!
        from openai import OpenAI

        client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
            # Optional: Add OpenRouter-specific headers
            default_headers={
                "HTTP-Referer": "https://genops-demo.com",
                "X-Title": "GenOps Auto-Instrumentation Demo",
            },
        )

        print("   âœ… OpenRouter client created (standard code)")

        print("\nğŸš€ Step 3: Make requests - telemetry is automatic!")

        test_requests = [
            {
                "name": "Fast & Cheap: Llama 3.2 3B",
                "model": "meta-llama/llama-3.2-3b-instruct",
                "prompt": "What is the capital of France?",
            },
            {
                "name": "Balanced: GPT-4o",
                "model": "openai/gpt-4o",
                "prompt": "Explain quantum computing in simple terms.",
            },
            {
                "name": "Reasoning: Claude 3.5 Sonnet",
                "model": "anthropic/claude-3-5-sonnet",
                "prompt": "What are the ethical considerations of AI in healthcare?",
            },
        ]

        total_tokens = 0
        successful_requests = 0

        for i, request in enumerate(test_requests, 1):
            print(f"\n   {i}. {request['name']}")
            print(f"      Model: {request['model']}")
            print(f"      Prompt: {request['prompt']}")

            try:
                # Standard OpenAI SDK call - GenOps automatically captures telemetry
                response = client.chat.completions.create(
                    model=request["model"],
                    messages=[{"role": "user", "content": request["prompt"]}],
                    max_tokens=80,
                )

                # Extract response
                content = response.choices[0].message.content
                usage = response.usage

                print(
                    f"      âœ… Success! Tokens: {usage.total_tokens}, Cost tracked automatically"
                )
                print(
                    f"      Response: {content[:60]}{'...' if len(content) > 60 else ''}"
                )

                total_tokens += usage.total_tokens
                successful_requests += 1

            except Exception as e:
                print(f"      âŒ Error: {str(e)}")

        print("\n" + "=" * 50)
        print("ğŸ“Š Auto-Instrumentation Results")
        print("=" * 50)
        print(f"âœ… Successful Requests: {successful_requests}/{len(test_requests)}")
        print(f"ğŸ“Š Total Tokens Used: {total_tokens}")
        print("ğŸ¯ Zero Code Changes Required!")

        print("\nğŸ” What GenOps Captured Automatically:")
        print("   â€¢ Request/response for each model")
        print("   â€¢ Token usage and cost calculations")
        print("   â€¢ Provider routing decisions (OpenAI vs Anthropic vs Meta)")
        print("   â€¢ Governance attributes (team, project, environment)")
        print("   â€¢ OpenTelemetry traces for observability integration")
        print("   â€¢ Multi-provider cost attribution")

        print("\nğŸ“ˆ Telemetry Attributes Added:")
        print("   â€¢ genops.service.name: openrouter-demo")
        print("   â€¢ genops.team: ai-platform-team")
        print("   â€¢ genops.project: multi-provider-experiment")
        print("   â€¢ genops.environment: development")
        print("   â€¢ genops.provider: openrouter")
        print("   â€¢ genops.openrouter.actual_provider: [varies by model]")
        print("   â€¢ genops.cost.total: [calculated per request]")

        print("\nğŸ”„ How It Works:")
        print("   1. genops.init() patches the OpenAI client globally")
        print("   2. When base_url contains 'openrouter.ai', GenOps intercepts")
        print("   3. Requests flow through GenOps telemetry layer")
        print("   4. Original response returned unchanged")
        print("   5. Telemetry exported to configured observability backend")

        print("\nâœ¨ Benefits:")
        print("   â€¢ No code changes to existing OpenRouter applications")
        print("   â€¢ Automatic cost tracking across 400+ models")
        print("   â€¢ Unified governance across all AI providers")
        print("   â€¢ Drop-in observability for existing systems")
        print("   â€¢ Multi-provider cost attribution and budgeting")

        print("\nğŸš€ Next Steps:")
        print(
            "   â€¢ Add per-request governance: client.chat.completions.create(..., team='new-team')"
        )
        print("   â€¢ Set up budget alerts in your observability dashboard")
        print("   â€¢ Try production_patterns.py for deployment best practices")

    except ImportError as e:
        print(f"âŒ Import Error: {e}")
        print("ğŸ’¡ Install required packages: pip install genops-ai openai")
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("ğŸ’¡ Check your API key and network connection")


def show_comparison():
    """Show before/after code comparison."""
    print("\nğŸ“‹ Code Comparison: Before vs After GenOps")
    print("=" * 50)

    print("âŒ BEFORE (No governance):")
    print("""
from openai import OpenAI

client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=key)
response = client.chat.completions.create(
    model="anthropic/claude-3-sonnet",
    messages=[{"role": "user", "content": "Hello"}]
)
# No cost tracking, no governance, no observability
""")

    print("âœ… AFTER (With GenOps):")
    print("""
import genops
genops.init()  # <-- Only addition needed!

from openai import OpenAI

client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=key)
response = client.chat.completions.create(
    model="anthropic/claude-3-sonnet",
    messages=[{"role": "user", "content": "Hello"}]
)
# Automatic cost tracking, governance attributes, full observability!
""")

    print("ğŸ¯ Result: 1 line addition = Complete AI governance")


if __name__ == "__main__":
    demonstrate_auto_instrumentation()
    show_comparison()
