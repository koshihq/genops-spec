# OpenAI-specific values for GenOps AI Helm chart

global:
  environment: production
  team: openai-team
  project: openai-service
  costCenter: ai-engineering

# Deployment configuration for OpenAI workloads
deployment:
  replicaCount: 3
  image:
    repository: genops/openai-service
    tag: "1.0.0"
  
  # OpenAI-specific resource requirements
  container:
    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
        ephemeral-storage: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
        ephemeral-storage: 1Gi
  
  # Pod labels for OpenAI service
  podLabels:
    ai-provider: openai
    service-type: llm-api
    cost-category: ai-inference

# Service configuration
service:
  type: ClusterIP
  port: 8000
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"

# OpenAI provider configuration
providers:
  openai:
    enabled: true
    apiKeySecret:
      name: openai-secrets
      key: api-key
    
    # Model configuration
    models:
      default: gpt-3.5-turbo
      allowed:
        - gpt-4
        - gpt-4-turbo
        - gpt-3.5-turbo
        - gpt-3.5-turbo-16k
        - text-embedding-3-small
        - text-embedding-3-large
        - text-embedding-ada-002
    
    # OpenAI-specific settings
    settings:
      organization: ""  # Optional: OpenAI org ID
      defaultMaxTokens: 4000
      defaultTemperature: 0.7
      timeoutSeconds: 300
      retryAttempts: 3
  
  # Disable other providers for OpenAI-focused deployment
  anthropic:
    enabled: false
  openrouter:
    enabled: false

# OpenTelemetry configuration for OpenAI telemetry
opentelemetry:
  enabled: true
  endpoint: "http://otel-collector:4318"
  serviceName: "genops-openai-service"
  serviceVersion: "1.0.0"
  
  # OpenAI-specific attributes
  resourceAttributes:
    - key: ai.provider
      value: openai
    - key: service.component
      value: llm-gateway

# Governance configuration for OpenAI
governance:
  defaultAttributes:
    team: openai-team
    project: openai-service
    aiProvider: openai
    costCenter: ai-engineering
  
  # OpenAI-specific policies
  policies:
    enabled: true
    
    # Cost management for OpenAI pricing
    costLimits:
      daily: 500.00    # Higher limits for production OpenAI usage
      monthly: 12000.00
      currency: USD
      enforcement: throttle
    
    # Rate limits matching OpenAI tiers
    rateLimits:
      requestsPerMinute: 60     # Tier 1 default
      requestsPerHour: 3000     # Tier 1 default
      burstLimit: 20
      enforcement: throttle
    
    # Content safety with OpenAI moderation
    contentSafety:
      enabled: true
      minimumScore: 0.85
      useOpenAIModerationAPI: true
      categories: ["hate", "harassment", "self-harm", "sexual", "violence"]
    
    # Data classification for OpenAI requests
    dataClassification:
      enabled: true
      allowedLevels: ["public", "internal", "confidential"]
      requireClassification: true

# Autoscaling configuration optimized for OpenAI workloads
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 15
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  
  # Custom metrics for OpenAI scaling
  customMetrics:
    enabled: true
    metrics:
    - type: Pods
      pods:
        metric:
          name: genops_openai_requests_per_second
        target:
          type: AverageValue
          averageValue: "15"
    - type: Pods
      pods:
        metric:
          name: genops_openai_queue_length
        target:
          type: AverageValue
          averageValue: "10"

# Monitoring configuration
monitoring:
  serviceMonitor:
    enabled: true
    interval: 15s
    scrapeTimeout: 10s
    labels:
      provider: openai
      component: ai-service
    metricRelabelings:
    # Add provider label to all metrics
    - sourceLabels: [__name__]
      targetLabel: ai_provider
      replacement: openai

# Network policy for OpenAI service security
networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  
  ingress:
    # Allow ingress from API gateway
    - from:
      - namespaceSelector:
          matchLabels:
            name: api-gateway
      - namespaceSelector:
          matchLabels:
            name: ingress-nginx
      ports:
      - protocol: TCP
        port: 8000
  
  egress:
    # Allow DNS
    - to: []
      ports:
      - protocol: UDP
        port: 53
    
    # Allow OpenAI API access
    - to: []
      ports:
      - protocol: TCP
        port: 443
      # OpenAI API endpoints
    
    # Allow OpenTelemetry collector
    - to:
      - namespaceSelector: {}
        podSelector:
          matchLabels:
            app.kubernetes.io/name: otel-collector
      ports:
      - protocol: TCP
        port: 4317
      - protocol: TCP
        port: 4318

# Secrets configuration
secrets:
  create: true
  name: openai-secrets
  apiKeys:
    openai: ""  # Set via --set secrets.apiKeys.openai=sk-...

# ConfigMap for OpenAI-specific configuration
configMap:
  create: true
  data:
    # OpenAI API configuration
    OPENAI_API_BASE: "https://api.openai.com/v1"
    OPENAI_DEFAULT_MODEL: "gpt-3.5-turbo"
    OPENAI_MAX_TOKENS: "4000"
    OPENAI_TEMPERATURE: "0.7"
    OPENAI_REQUEST_TIMEOUT: "300"
    OPENAI_MAX_RETRIES: "3"
    
    # Governance configuration
    ENABLE_COST_TRACKING: "true"
    ENABLE_CONTENT_MODERATION: "true"
    ENABLE_RATE_LIMITING: "true"
    
    # Logging configuration
    LOG_LEVEL: "info"
    LOG_FORMAT: "json"
    ENABLE_REQUEST_LOGGING: "true"

# Health checks optimized for OpenAI API calls
livenessProbe:
  enabled: true
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 30
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  enabled: true
  httpGet:
    path: /ready
    port: 8000
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

startupProbe:
  enabled: true
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 15
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 15

# Pod disruption budget for high availability
deployment:
  podDisruptionBudget:
    enabled: true
    minAvailable: 2

# Security context for OpenAI service
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  capabilities:
    drop:
    - ALL
  seccompProfile:
    type: RuntimeDefault

# Environment-specific overrides
environments:
  production:
    replicaCount: 3
    autoscaling:
      minReplicas: 3
      maxReplicas: 15
    governance:
      policies:
        costLimits:
          daily: 500.00
          enforcement: throttle
        rateLimits:
          requestsPerMinute: 60
  
  staging:
    replicaCount: 2
    autoscaling:
      minReplicas: 2
      maxReplicas: 5
    governance:
      policies:
        costLimits:
          daily: 100.00
          enforcement: warn
        rateLimits:
          requestsPerMinute: 30
  
  development:
    replicaCount: 1
    autoscaling:
      enabled: false
    governance:
      policies:
        costLimits:
          daily: 20.00
          enforcement: warn
        rateLimits:
          requestsPerMinute: 10