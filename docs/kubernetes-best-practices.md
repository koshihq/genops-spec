# Production Deployment Best Practices for GenOps AI

> **Status:** ğŸ“‹ Documentation in progress
> **Last Updated:** 2026-01-18

Comprehensive best practices for deploying and operating GenOps AI in production Kubernetes environments.

---

## Overview

Production-ready Kubernetes deployments require careful attention to reliability, security, performance, and operational excellence. This guide covers:
- **Deployment Architecture** patterns for resilient AI workloads
- **Resource Management** strategies for optimal performance and cost
- **Security Hardening** to protect sensitive AI operations
- **Operational Excellence** with monitoring, alerting, and incident response
- **Cost Optimization** to maximize ROI on AI infrastructure

---

## Quick Reference: Production Readiness Checklist

### Essential Requirements

**âœ… High Availability:**
- [ ] Multi-zone deployment (minimum 3 zones)
- [ ] Minimum 3 replicas for critical services
- [ ] Pod Disruption Budgets configured
- [ ] Health checks and auto-recovery enabled

**âœ… Security:**
- [ ] RBAC with least-privilege access
- [ ] Pod Security Standards enforced
- [ ] Network policies for traffic isolation
- [ ] Secrets management with external store
- [ ] Regular security scanning and updates

**âœ… Observability:**
- [ ] Metrics collection and dashboards
- [ ] Distributed tracing configured
- [ ] Centralized logging
- [ ] Alerting on critical metrics
- [ ] SLO tracking and reporting

**âœ… Governance:**
- [ ] Cost attribution by team/project/customer
- [ ] Budget limits and alerts configured
- [ ] Policy enforcement for AI operations
- [ ] Compliance tracking and reporting

**âœ… Disaster Recovery:**
- [ ] Regular automated backups
- [ ] Multi-region failover capability
- [ ] Documented recovery procedures
- [ ] Regular DR testing (quarterly)

---

## Table of Contents

### Planned Documentation Sections

1. **Architecture Best Practices**
   - Microservices vs monolithic design
   - Stateless vs stateful services
   - Multi-zone and multi-region patterns
   - Service mesh considerations
   - Load balancing strategies

2. **Resource Management**
   - CPU and memory sizing guidelines
   - GPU allocation for AI workloads
   - Resource requests and limits
   - Autoscaling strategies (HPA, VPA, Cluster Autoscaler)
   - Node selection and affinity

3. **Security Best Practices**
   - Zero-trust networking
   - Identity and access management
   - Secret rotation strategies
   - Compliance frameworks (SOC2, HIPAA, PCI-DSS)
   - Vulnerability management

4. **Observability and Monitoring**
   - Key metrics for AI workloads
   - Dashboard design principles
   - Alerting strategies and thresholds
   - Log aggregation and analysis
   - Distributed tracing patterns

5. **Performance Optimization**
   - Container image optimization
   - Network performance tuning
   - Storage performance considerations
   - Caching strategies
   - Database optimization

6. **Cost Management**
   - Right-sizing resources
   - Spot/preemptible instance usage
   - Reserved capacity strategies
   - Cost allocation and chargeback
   - Waste identification and elimination

7. **Operational Excellence**
   - GitOps and Infrastructure as Code
   - CI/CD best practices
   - Incident response procedures
   - Change management processes
   - Documentation and runbooks

8. **Scaling Strategies**
   - Horizontal vs vertical scaling
   - Predictive autoscaling
   - Traffic management during scale events
   - Database scaling patterns
   - Cost-aware scaling

---

## Related Documentation

**Kubernetes Guides:**
- [Kubernetes Getting Started](kubernetes-getting-started.md)
- [Security Hardening](kubernetes-security.md)
- [Cost Optimization](kubernetes-cost-optimization.md)
- [Disaster Recovery](kubernetes-dr.md)

**Advanced Topics:**
- [Multi-Tenant Architecture](kubernetes-multi-tenant.md)
- [Service Mesh Integration](kubernetes-service-mesh.md)
- [Advanced Observability](kubernetes-observability.md)

---

## Key Best Practices

### 1. Resource Configuration

**Always Set Resource Requests and Limits:**
```yaml
containers:
- name: genops-ai
  resources:
    requests:
      cpu: "1000m"      # Guaranteed CPU
      memory: "2Gi"     # Guaranteed memory
    limits:
      cpu: "2000m"      # Max CPU (can burst)
      memory: "4Gi"     # Hard memory limit
```

**Use Vertical Pod Autoscaler for Recommendations:**
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: genops-ai-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: genops-ai
  updatePolicy:
    updateMode: "Off"  # Recommendation-only mode
```

### 2. High Availability Configuration

**Multi-Zone Deployment:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: genops-ai
spec:
  replicas: 6  # Minimum 3, ideally 6+ for 3 zones

  template:
    spec:
      # Spread evenly across zones
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: genops-ai

      # Avoid co-location on same node
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - genops-ai
            topologyKey: kubernetes.io/hostname
```

**Pod Disruption Budget:**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: genops-ai-pdb
spec:
  minAvailable: 3  # Always maintain 3 pods
  selector:
    matchLabels:
      app: genops-ai
```

### 3. Health Checks

**Comprehensive Probes:**
```yaml
containers:
- name: genops-ai
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3

  startupProbe:
    httpGet:
      path: /startup
      port: 8080
    initialDelaySeconds: 0
    periodSeconds: 10
    failureThreshold: 30  # Allow up to 5 minutes for startup
```

### 4. Autoscaling Strategy

**Horizontal Pod Autoscaler (HPA):**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: genops-ai-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: genops-ai
  minReplicas: 3
  maxReplicas: 20
  metrics:
  # CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # Memory-based scaling
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Custom metric: requests per second
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50  # Scale down max 50% of pods at a time
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
      - type: Percent
        value: 100  # Double pods if needed
        periodSeconds: 30
      - type: Pods
        value: 4  # Or add 4 pods
        periodSeconds: 30
      selectPolicy: Max
```

### 5. Container Image Best Practices

**Optimized Dockerfile:**
```dockerfile
# Use minimal base image
FROM python:3.11-slim

# Create non-root user
RUN groupadd -r genops && useradd -r -g genops genops

# Install dependencies in separate layer for caching
COPY requirements.txt /app/
RUN pip install --no-cache-dir -r /app/requirements.txt

# Copy application code
COPY --chown=genops:genops . /app/
WORKDIR /app

# Switch to non-root user
USER genops

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=30s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8080/health')"

# Run application
CMD ["python", "main.py"]
```

**Image Tagging Strategy:**
- Use semantic versioning (e.g., `v1.2.3`)
- Tag with Git commit SHA for traceability
- Never use `latest` in production
- Implement image scanning in CI/CD

### 6. Configuration Management

**ConfigMaps for Non-Sensitive Config:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: genops-config
  namespace: genops
data:
  ENVIRONMENT: "production"
  LOG_LEVEL: "info"
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector.observability:4318"
  GENOPS_TEAM: "platform-engineering"
```

**Secrets for Sensitive Data:**
```yaml
# Use external secret management (Vault, AWS Secrets Manager)
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: genops-secrets
  namespace: genops
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secrets-manager
    kind: SecretStore
  target:
    name: genops-api-keys
  data:
  - secretKey: openai-api-key
    remoteRef:
      key: prod/genops/openai
```

### 7. Network Policies

**Default Deny with Selective Allow:**
```yaml
# Default deny all
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: genops
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# Allow specific ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: genops-ai-ingress
  namespace: genops
spec:
  podSelector:
    matchLabels:
      app: genops-ai
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
  # OTLP exporter
  - to:
    - namespaceSelector:
        matchLabels:
          name: observability
    ports:
    - protocol: TCP
      port: 4318
  # HTTPS to external services
  - to:
    - podSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

### 8. Monitoring and Alerting

**Critical Alerts:**
```yaml
# Prometheus alert rules
apiVersion: v1
kind: ConfigMap
metadata:
  name: genops-alert-rules
  namespace: monitoring
data:
  genops-alerts.yaml: |
    groups:
    - name: genops-critical
      interval: 30s
      rules:
      # Service availability
      - alert: GenOpsServiceDown
        expr: up{job="genops-ai"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "GenOps AI service is down"
          description: "Service {{ $labels.instance }} has been down for more than 2 minutes."

      # High error rate
      - alert: GenOpsHighErrorRate
        expr: rate(http_requests_total{job="genops-ai",status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes."

      # Budget exceeded
      - alert: GenOpsBudgetExceeded
        expr: genops_budget_consumed_percent > 95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Budget threshold exceeded"
          description: "Team {{ $labels.team }} has consumed {{ $value }}% of their budget."

      # High latency
      - alert: GenOpsHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="genops-ai"}[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High request latency"
          description: "P95 latency is {{ $value | humanizeDuration }} for the last 5 minutes."
```

### 9. GitOps and IaC

**Repository Structure:**
```
genops-infrastructure/
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ configmap.yaml
â”œâ”€â”€ overlays/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ patches/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ patches/
â”‚   â””â”€â”€ production/
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â””â”€â”€ patches/
â”œâ”€â”€ helm-values/
â”‚   â”œâ”€â”€ dev-values.yaml
â”‚   â”œâ”€â”€ staging-values.yaml
â”‚   â””â”€â”€ prod-values.yaml
â””â”€â”€ README.md
```

**ArgoCD Application:**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: genops-ai-prod
  namespace: argocd
spec:
  project: production
  source:
    repoURL: https://github.com/your-org/genops-infrastructure
    targetRevision: main
    path: overlays/production
  destination:
    server: https://kubernetes.default.svc
    namespace: genops
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

### 10. Cost Optimization

**Resource Right-Sizing:**
- Monitor actual usage with metrics
- Use VPA recommendations
- Adjust requests to match reality
- Set appropriate limits to prevent OOM

**Cluster Autoscaling:**
```yaml
# Enable cluster autoscaler
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
  namespace: kube-system
data:
  priorities: |-
    10:
      - .*-spot-.*  # Prefer spot instances
    50:
      - .*-ondemand-.*  # Then on-demand
```

**Cost Attribution:**
```python
# GenOps tracks costs automatically
from genops import track_usage

@track_usage(
    team="ml-platform",
    project="production-inference",
    customer_id="enterprise-123",
    cost_center="engineering",
    budget_limit=1000.0,
    budget_period="monthly"
)
def ai_operation():
    # Costs automatically tracked and attributed
    response = model.generate(prompt)
    return response
```

---

## Production Readiness Matrix

| Category | Basic | Production | Enterprise |
|----------|-------|-----------|------------|
| **Availability** | Single zone | Multi-zone (3+) | Multi-region |
| **Replicas** | 1-2 | 3-6 | 6+ |
| **Autoscaling** | Manual | HPA | HPA + VPA + CA |
| **Monitoring** | Basic metrics | Full observability | APM + Tracing |
| **Security** | Basic RBAC | Pod Security + NP | Zero-trust |
| **DR** | Manual backup | Automated backup | Multi-region DR |
| **Cost Management** | Basic tracking | Budget alerts | FinOps integration |

---

## Deployment Checklist by Environment

### Development
- [ ] Single replica acceptable
- [ ] Basic resource limits
- [ ] Development-level logging
- [ ] Cost tracking enabled
- [ ] No strict SLAs

### Staging
- [ ] Multi-replica (2-3)
- [ ] Production-like configuration
- [ ] Full monitoring and logging
- [ ] Test DR procedures
- [ ] Performance testing

### Production
- [ ] All items from Production Readiness Checklist
- [ ] Multi-zone deployment
- [ ] Autoscaling configured
- [ ] Security hardening complete
- [ ] DR tested and validated
- [ ] On-call rotation established
- [ ] Runbooks documented

---

## Architecture Best Practices for AI Workloads

### Microservices Patterns

**Decomposition Strategies:**
```
AI Application Architecture:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Gateway                            â”‚
â”‚    (Kong/NGINX - Authentication, Rate Limiting)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                             â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Inference API    â”‚         â”‚   Admin API         â”‚
       â”‚  (FastAPI)        â”‚         â”‚   (Management)      â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Model A â”‚ â”‚Model â”‚  â”‚ Context   â”‚
â”‚Service â”‚ â”‚  B   â”‚  â”‚ Enrichmentâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  GenOps Telemetry  â”‚
    â”‚  (OTLP Collector)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### GPU Allocation for AI Workloads

**GPU Node Pool Configuration:**
```yaml
# GPU workload deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-inference
  namespace: genops
spec:
  replicas: 2
  template:
    spec:
      tolerations:
      - key: nvidia.com/gpu
        operator: Equal
        value: "true"
        effect: NoSchedule

      nodeSelector:
        nvidia.com/gpu: "true"
        gpu-type: tesla-v100

      containers:
      - name: inference
        image: genops-gpu-inference:latest
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1

        env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
```

### Advanced Autoscaling with Multiple Metrics

**Combined Autoscaling Strategy:**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: genops-advanced-hpa
  namespace: genops
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: genops-ai-inference
  minReplicas: 3
  maxReplicas: 50
  metrics:
  # Multiple resource metrics
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # Custom application metrics
  - type: Pods
    pods:
      metric:
        name: http_requests_queued
      target:
        type: AverageValue
        averageValue: "10"

  # Scale-up/down policies
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 4
        periodSeconds: 60
      selectPolicy: Max

    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

---

## Performance Optimization Techniques

### Container Image Optimization

**Multi-Stage Build Example:**
```dockerfile
FROM python:3.11-slim as builder

WORKDIR /build

# Install build dependencies
RUN apt-get update && apt-get install -y gcc g++ && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

---

FROM python:3.11-slim

# Create non-root user
RUN useradd -m -u 1000 genops

# Copy dependencies from builder
COPY --from=builder /root/.local /home/genops/.local

# Copy application
WORKDIR /app
COPY --chown=genops:genops . .

USER genops

ENV PATH=/home/genops/.local/bin:$PATH

EXPOSE 8080
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
```

### Application Caching Strategies

**Multi-Tier Caching:**
```python
from redis import Redis
from functools import lru_cache
from genops import track_usage
import hashlib

redis_client = Redis(host='redis.genops', port=6379)

@lru_cache(maxsize=1000)
def get_embedding_cached(text: str):
    """In-memory LRU cache."""
    return compute_embedding(text)

@track_usage(team="ml-platform", project="inference-api")
def get_inference_result(prompt: str):
    """Multi-tier cached inference."""
    cache_key = f"inference:{hashlib.sha256(prompt.encode()).hexdigest()}"

    # Check Redis cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)

    # Generate new result
    result = model.generate(prompt)

    # Store in Redis with 1-hour TTL
    redis_client.setex(cache_key, 3600, json.dumps(result))

    return result
```

---

## Security Hardening Implementation

### Pod Security Standards Enforcement

**Restricted Security Context:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-inference
  namespace: genops-production
spec:
  template:
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: inference
        image: genops-ai:latest
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true

        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /cache

      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
```

### Network Policies for Zero-Trust

**Complete Network Segmentation:**
```yaml
# 1. Default deny all
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: genops
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# 2. Allow DNS
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: genops
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53

---
# 3. Allow ingress from gateway
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-gateway
  namespace: genops
spec:
  podSelector:
    matchLabels:
      app: genops-ai-inference
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: api-gateway
    ports:
    - protocol: TCP
      port: 8080
```

---

## Cost Management and FinOps

### Resource Right-Sizing

**VPA Recommendations:**
```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: genops-ai-vpa
  namespace: genops
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: genops-ai-inference

  updatePolicy:
    updateMode: "Auto"

  resourcePolicy:
    containerPolicies:
    - containerName: inference
      minAllowed:
        cpu: "500m"
        memory: "1Gi"
      maxAllowed:
        cpu: "8"
        memory: "16Gi"
```

### Spot Instance Strategies

**Mixed Spot and On-Demand:**
```yaml
# Spot node group for cost savings
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
nodeGroups:
  - name: batch-spot
    instancesDistribution:
      instanceTypes:
      - c5.2xlarge
      - c5.4xlarge
      - m5.2xlarge
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0
      spotAllocationStrategy: capacity-optimized
    desiredCapacity: 10
    minSize: 0
    maxSize: 50
    labels:
      workload-type: batch
      lifecycle: spot
```

---

## Operational Excellence

### GitOps with ArgoCD

**Application Definition:**
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: genops-ai-prod
  namespace: argocd
spec:
  project: production

  source:
    repoURL: https://github.com/genops/deployments
    targetRevision: main
    path: kubernetes/production

  destination:
    server: https://kubernetes.default.svc
    namespace: genops

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

### Incident Response Runbooks

**On-Call Procedures:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: incident-runbook
  namespace: genops
data:
  ONCALL.md: |
    # GenOps AI On-Call Runbook

    ## Alert: High Error Rate

    **Severity:** P1

    ### Investigation
    ```bash
    kubectl logs -l app=genops-ai --tail=100 | grep ERROR
    kubectl get pods -n genops -o wide
    ```

    ### Mitigation
    ```bash
    # Rollback deployment
    kubectl rollout undo deployment/genops-ai -n genops
    kubectl rollout status deployment/genops-ai -n genops
    ```
```

---

## Comprehensive Production Readiness Checklist

### Infrastructure
- [ ] Multi-zone deployment configured and tested
- [ ] Auto-scaling policies validated under load
- [ ] Resource quotas and limits enforced
- [ ] Network policies implemented and tested
- [ ] TLS/mTLS enabled for all services
- [ ] Backup and restore procedures verified
- [ ] DR failover tested successfully

### Security
- [ ] Pod Security Standards enforced (restricted)
- [ ] RBAC policies reviewed and minimized
- [ ] Secrets encrypted at rest and in transit
- [ ] Network egress restricted to required endpoints
- [ ] Runtime security monitoring (Falco) enabled
- [ ] Container images scanned for vulnerabilities
- [ ] Supply chain security (Cosign, SBOM) implemented
- [ ] Security audit completed and passed

### Observability
- [ ] Prometheus metrics exported and validated
- [ ] Distributed tracing configured (Jaeger/Tempo)
- [ ] Centralized logging operational (Loki/ELK)
- [ ] Alerting rules defined and tested
- [ ] SLO/SLI dashboards created and reviewed
- [ ] GenOps governance telemetry validated
- [ ] On-call rotations established

### Performance
- [ ] Load testing completed (target RPS achieved)
- [ ] Resource sizing optimized based on metrics
- [ ] Caching strategy implemented and validated
- [ ] Database query performance optimized
- [ ] CDN configured for static assets
- [ ] Network latency within acceptable bounds

### Reliability
- [ ] Chaos engineering tests passed
- [ ] DR procedures documented and tested quarterly
- [ ] PodDisruptionBudgets configured correctly
- [ ] Health checks comprehensive (startup/liveness/readiness)
- [ ] Circuit breakers implemented for external dependencies
- [ ] Retry policies with exponential backoff configured

### Cost Management
- [ ] Cost tracking instrumented across all workloads
- [ ] Budget alerts configured and tested
- [ ] Resource right-sizing completed
- [ ] Spot instance strategy implemented where applicable
- [ ] FinOps review completed with stakeholders
- [ ] Cost allocation by team/project/customer validated

### Operations
- [ ] GitOps workflow operational and tested
- [ ] CI/CD pipeline validated end-to-end
- [ ] Runbooks documented and accessible
- [ ] On-call rotation established with coverage
- [ ] Incident response procedures tested
- [ ] Documentation complete and up-to-date
- [ ] Team training completed

---

## Production Deployment Matrix

| Category | Development | Staging | Production | Enterprise |
|----------|------------|---------|------------|------------|
| **Replicas** | 1 | 2-3 | 3-6 | 6+ |
| **Zones** | Single | Single | Multi (3+) | Multi-region |
| **Autoscaling** | None | HPA | HPA + VPA | HPA + VPA + CA |
| **Monitoring** | Basic | Full stack | APM + Tracing | Complete observability |
| **Security** | Basic | Standard | Hardened | Zero-trust |
| **DR** | None | Daily backup | Multi-region | Active-active |
| **SLA** | None | 95% | 99.9% | 99.99% |

---

## Next Steps

Ready to deploy GenOps AI to production? Follow this path:

1. **Review This Guide** - Understand all best practices
2. **Complete Readiness Checklist** - Verify all requirements met
3. **Deploy to Staging** - Test full configuration
4. **Perform Load Testing** - Validate performance and scaling
5. **Execute DR Drill** - Verify backup and recovery
6. **Security Audit** - Run compliance scans
7. **Deploy to Production** - Follow change management process
8. **Monitor and Iterate** - Continuous improvement

Return to [Kubernetes Getting Started](kubernetes-getting-started.md) for the complete deployment overview.

---

## Support

- **Documentation:** [GenOps AI Docs](https://github.com/KoshiHQ/GenOps-AI/tree/main/docs)
- **Issues:** [GitHub Issues](https://github.com/KoshiHQ/GenOps-AI/issues)
- **Community:** [Discussions](https://github.com/KoshiHQ/GenOps-AI/discussions)
