# MLflow Integration Guide

Complete guide for integrating GenOps governance with MLflow experiment tracking and model registry.

## Table of Contents

- [Overview](#overview)
- [Why MLflow + GenOps](#why-mlflow--genops)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Core Concepts](#core-concepts)
- [Architecture](#architecture)
- [Usage Patterns](#usage-patterns)
- [Cost Tracking](#cost-tracking)
- [Governance Attributes](#governance-attributes)
- [Advanced Features](#advanced-features)
- [API Reference](#api-reference)
- [Examples](#examples)
- [Troubleshooting](#troubleshooting)
- [Performance Considerations](#performance-considerations)
- [Best Practices](#best-practices)

---

## Overview

The GenOps MLflow provider enables comprehensive governance telemetry for MLflow experiment tracking and model registry operations. It extends MLflow with cost tracking, governance attribution, and OpenTelemetry-native observability without requiring changes to your existing MLflow code.

### Key Features

- **Zero-Code Auto-Instrumentation**: Enable governance with a single function call
- **Cost Tracking**: Automatic cost calculation for all MLflow operations
- **Multi-Level Attribution**: Team, project, customer, and cost center tracking
- **OpenTelemetry Native**: Standard OTLP export to your existing observability stack
- **Comprehensive Validation**: Built-in diagnostics with actionable fix suggestions
- **Production Ready**: Tested patterns for enterprise deployment

### What Gets Tracked

**Experiment Operations:**
- Experiment creation and configuration
- Run lifecycle management
- Parameter and metric logging
- Artifact and model storage
- Model registry operations

**Governance Telemetry:**
- Real-time cost attribution
- Team and project tracking
- Customer-level cost allocation
- Environment segregation (dev/staging/prod)
- Complete audit trail

**Performance Metrics:**
- Operation latency and throughput
- Storage utilization
- Cost optimization opportunities
- Resource usage patterns

---

## Why MLflow + GenOps

### The Challenge

MLflow provides excellent experiment tracking and model registry capabilities, but lacks native governance features:

- **Cost Visibility**: No built-in cost tracking or attribution
- **Multi-Tenant Isolation**: Limited support for customer-level tracking
- **Budget Control**: No native budget enforcement or alerting
- **Compliance**: Missing audit trails for governance requirements
- **Cross-Stack**: Doesn't integrate with enterprise observability platforms

### The GenOps Solution

GenOps extends MLflow with governance capabilities while maintaining full compatibility:

**For ML Engineers:**
- Zero code changes to existing MLflow workflows
- Automatic cost tracking without manual instrumentation
- Clear visibility into experiment costs and resource usage

**For Platform Teams:**
- Unified governance across all AI/ML tools
- Standard OpenTelemetry integration with existing observability
- Centralized policy enforcement and budget management

**For Finance/FinOps:**
- Accurate cost attribution by team, project, and customer
- Chargeback and showback capabilities
- Budget tracking and cost optimization insights

### Value Proposition

| Without GenOps | With GenOps |
|----------------|-------------|
| Manual cost estimation | Automatic real-time cost tracking |
| Team-level attribution | Customer-level granularity |
| Siloed observability | Unified OpenTelemetry telemetry |
| Reactive compliance | Proactive policy enforcement |
| Manual reporting | Automated governance dashboards |

---

## Installation

### Prerequisites

- Python 3.8 or higher
- MLflow 2.0 or higher (recommended: 2.9+)
- OpenTelemetry SDK (installed with GenOps)

### Install GenOps with MLflow Support

```bash
# Option 1: Install from source (development)
pip install -e .

# Option 2: Install from PyPI (when published)
pip install genops[mlflow]

# Option 3: Install with all optional dependencies
pip install genops[all]
```

### Verify Installation

```bash
python -c "from genops.providers.mlflow import instrument_mlflow; print('MLflow provider installed')"
```

### Install MLflow (if not already installed)

```bash
pip install mlflow
```

---

## Quick Start

### 1. Zero-Code Auto-Instrumentation

The fastest way to add governance to existing MLflow code:

```python
from genops.providers.mlflow import auto_instrument_mlflow
import mlflow

# Enable governance with one line
auto_instrument_mlflow()

# Your existing MLflow code works automatically with governance!
mlflow.set_experiment("my-experiment")

with mlflow.start_run():
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_metric("accuracy", 0.95)
    mlflow.log_artifact("model.pkl")
```

**What you get automatically:**
- Cost tracking for all operations
- Governance attributes on every run
- OpenTelemetry traces exported
- Team/project attribution from environment variables

### 2. Manual Instrumentation with Explicit Governance

For more control over governance attributes:

```python
from genops.providers.mlflow import instrument_mlflow
import mlflow

# Create adapter with explicit governance
adapter = instrument_mlflow(
    tracking_uri="http://localhost:5000",
    team="ml-team",
    project="model-optimization",
    environment="development",
    customer_id="customer-001"
)

# Track MLflow run with governance context
with adapter.track_mlflow_run(
    experiment_name="optimization-experiment",
    run_name="run-001"
) as run:
    # Log parameters
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("batch_size", 32)

    # Log metrics
    mlflow.log_metric("train_accuracy", 0.92)
    mlflow.log_metric("val_accuracy", 0.89)

    # Log artifacts
    mlflow.log_artifact("model_summary.txt")

# Check governance metrics
metrics = adapter.get_metrics()
print(f"Total cost: ${metrics['daily_usage']:.6f}")
print(f"Operations tracked: {metrics['operation_count']}")
```

### 3. Validate Your Setup

```bash
python examples/mlflow/setup_validation.py
```

Expected output:
```
âœ… PASSED - You're ready to use MLflow with GenOps!

ðŸ“¦ Dependencies:
  âœ… mlflow
  âœ… opentelemetry
  âœ… genops

âš™ï¸  Configuration:
  â€¢ tracking_uri: http://localhost:5000
  â€¢ genops_team: ml-team
  â€¢ genops_project: model-optimization
```

---

## Core Concepts

### 1. Adapter Pattern

The `GenOpsMLflowAdapter` is the main interface for MLflow governance:

```python
from genops.providers.mlflow import GenOpsMLflowAdapter

adapter = GenOpsMLflowAdapter(
    tracking_uri="http://localhost:5000",  # MLflow tracking server
    registry_uri="http://localhost:5000",   # Model registry (optional)
    team="ml-team",                         # Team attribution
    project="model-training",               # Project tracking
    customer_id="customer-123",             # Customer attribution (optional)
    environment="production"                # Environment (dev/staging/prod)
)
```

### 2. Context Managers

GenOps uses context managers for operation tracking:

```python
# Track complete MLflow run lifecycle
with adapter.track_mlflow_run(
    experiment_name="my-experiment",
    run_name="my-run",
    customer_id="customer-456"  # Override customer for this run
) as run:
    # Your MLflow operations here
    pass
# Automatic finalization and cost calculation
```

### 3. Cost Tracking

All operations are automatically cost-tracked:

```python
# Costs are calculated based on operation type:
mlflow.log_param("param", "value")     # $0.0001 (tracking API)
mlflow.log_metric("metric", 0.95)      # $0.0001 (tracking API)
mlflow.log_artifact("file.txt")        # Size-based (storage backend)
mlflow.log_model(model, "model")       # Size-based (storage backend)
mlflow.register_model(uri, "name")     # $0.0005 (registry operation)
```

### 4. Governance Attributes

Attributes propagate automatically to all operations:

```python
adapter = instrument_mlflow(
    team="ml-team",              # Required for cost attribution
    project="model-training",    # Required for project tracking
    customer_id="customer-123",  # Optional: multi-tenant tracking
    environment="production",    # Optional: environment segregation
    cost_center="ml-research"    # Optional: financial reporting
)
```

These attributes appear as tags in MLflow UI:
- `genops.team` = ml-team
- `genops.project` = model-training
- `genops.customer_id` = customer-123
- `genops.environment` = production
- `genops.cost_center` = ml-research

---

## Architecture

### Design Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Your Application                    â”‚
â”‚                                                      â”‚
â”‚  import mlflow                                       â”‚
â”‚  mlflow.log_param(...)  â† Zero code changes!       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GenOpsMLflowAdapter                       â”‚
â”‚                                                      â”‚
â”‚  â€¢ Wraps MLflow methods                            â”‚
â”‚  â€¢ Adds governance context                         â”‚
â”‚  â€¢ Tracks costs                                     â”‚
â”‚  â€¢ Exports telemetry                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLflow  â”‚  â”‚  Cost  â”‚  â”‚ OpenTelemetryâ”‚
â”‚  Server  â”‚  â”‚  Track â”‚  â”‚   Exporter   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Your Observability â”‚
                    â”‚  Platform (Datadog, â”‚
                    â”‚  Grafana, etc.)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Architecture

**1. GenOpsMLflowAdapter** (`adapter.py`)
- Inherits from `BaseFrameworkProvider`
- Implements 10 abstract methods
- Wraps MLflow methods with governance
- Manages instrumentation lifecycle

**2. MLflowCostAggregator** (`cost_aggregator.py`)
- Tracks costs at run, experiment, and project levels
- Calculates costs based on operation type and storage backend
- Provides cost summaries and reports
- Singleton pattern for global aggregation

**3. Validation Framework** (`validation.py`)
- Validates dependencies (mlflow, opentelemetry, genops)
- Checks configuration (tracking URI, governance attributes)
- Tests connectivity (tracking server, registry)
- Provides actionable fix suggestions

**4. Registration System** (`registration.py`)
- Auto-detects MLflow configuration from environment
- Registers provider with GenOps instrumentation system
- Enables zero-code auto-instrumentation
- Manages provider lifecycle

### Instrumentation Mechanism

GenOps uses **wrapper-based patching** (non-invasive):

```python
# Original MLflow method
original_log_param = mlflow.log_param

# GenOps wrapper
def wrapped_log_param(key, value):
    # 1. Extract governance context
    # 2. Start OpenTelemetry span
    # 3. Call original method
    result = original_log_param(key, value)
    # 4. Record cost and metrics
    # 5. Return result
    return result

# Replace method
mlflow.log_param = wrapped_log_param
```

**Advantages:**
- No changes to MLflow source code
- Clean unpatch restores originals
- Full compatibility with MLflow versions
- No performance impact when not instrumented

---

## Usage Patterns

### Pattern 1: Environment-Based Configuration

Use environment variables for zero-code setup:

```bash
# Set governance attributes
export GENOPS_TEAM="ml-team"
export GENOPS_PROJECT="model-optimization"
export GENOPS_ENVIRONMENT="development"
export GENOPS_CUSTOMER_ID="customer-001"

# Set MLflow configuration
export MLFLOW_TRACKING_URI="http://localhost:5000"
```

```python
from genops.providers.mlflow import auto_instrument_mlflow
import mlflow

# Auto-instrument with environment config
auto_instrument_mlflow()

# All MLflow operations automatically have governance
with mlflow.start_run():
    mlflow.log_param("param", "value")
```

### Pattern 2: Explicit Configuration

Set governance attributes programmatically:

```python
from genops.providers.mlflow import instrument_mlflow

adapter = instrument_mlflow(
    tracking_uri="http://localhost:5000",
    team="ml-team",
    project="model-training"
)

# Track run with explicit governance
with adapter.track_mlflow_run(
    experiment_name="training-exp",
    run_name="run-001"
) as run:
    # Your MLflow operations
    pass
```

### Pattern 3: Per-Run Override

Override governance attributes for specific runs:

```python
adapter = instrument_mlflow(team="ml-team", project="default-project")

# Run 1: Default governance
with adapter.track_mlflow_run(
    experiment_name="exp1",
    run_name="run1"
) as run:
    pass

# Run 2: Override customer
with adapter.track_mlflow_run(
    experiment_name="exp2",
    run_name="run2",
    customer_id="customer-specific"  # Override for this run
) as run:
    pass
```

### Pattern 4: Hierarchical Runs

Track parent-child run relationships:

```python
# Parent run
with adapter.track_mlflow_run(
    experiment_name="parent-exp",
    run_name="parent-run"
) as parent_run:
    mlflow.log_param("parent_param", "value")

    # Child run 1
    with adapter.track_mlflow_run(
        experiment_name="parent-exp",
        run_name="child-run-1",
        parent_run_id=parent_run.info.run_id
    ) as child_run_1:
        mlflow.log_metric("child1_metric", 0.8)

    # Child run 2
    with adapter.track_mlflow_run(
        experiment_name="parent-exp",
        run_name="child-run-2",
        parent_run_id=parent_run.info.run_id
    ) as child_run_2:
        mlflow.log_metric("child2_metric", 0.9)

# Costs automatically aggregate to parent
```

### Pattern 5: Model Registry Workflow

Track model registration and deployment:

```python
with adapter.track_mlflow_run(
    experiment_name="model-training",
    run_name="training-run"
) as run:
    # Train and log model
    mlflow.sklearn.log_model(model, "model")

    # Register model
    model_uri = f"runs:/{run.info.run_id}/model"
    mlflow.register_model(model_uri, "my-model")

# All costs tracked: training, storage, registry
```

---

## Cost Tracking

### Cost Model

GenOps tracks costs across multiple tiers:

#### 1. Tracking API Calls
**Cost: $0.0001 per operation**

Operations counted:
- `log_param()` - Parameter logging
- `log_metric()` - Metric logging
- `set_tag()` - Tag operations
- `start_run()` - Run creation
- Experiment operations

```python
mlflow.log_param("learning_rate", 0.01)  # $0.0001
mlflow.log_metric("accuracy", 0.95)      # $0.0001
mlflow.set_tag("version", "1.0")         # $0.0001
```

#### 2. Artifact Storage
**Cost: Backend-specific, size-based**

Storage backends:
- **Local**: Free
- **S3**: $0.023 per GB-month (prorated daily)
- **Azure Blob**: $0.020 per GB-month
- **Google Cloud Storage**: $0.020 per GB-month

```python
# 10 MB artifact to S3
mlflow.log_artifact("file.txt")  # ~$0.0000075 per day
# Calculation: (10 MB / 1024 GB) * $0.023 / 30 days
```

#### 3. Model Storage
**Cost: Same as artifact storage**

```python
# 500 MB model to S3
mlflow.sklearn.log_model(model, "model")  # ~$0.000375 per day
# Calculation: (500 MB / 1024 GB) * $0.023 / 30 days
```

#### 4. Model Registry
**Cost: $0.0005 per operation**

Registry operations:
- `register_model()` - Model registration
- `transition_model_version_stage()` - Stage transitions
- Model version operations

```python
mlflow.register_model(model_uri, "my-model")  # $0.0005
```

### Cost Retrieval

**Get Current Costs:**
```python
metrics = adapter.get_metrics()
print(f"Daily usage: ${metrics['daily_usage']:.6f}")
print(f"Operation count: {metrics['operation_count']}")
```

**Get Detailed Cost Breakdown:**
```python
from genops.providers.mlflow import create_mlflow_cost_context

with create_mlflow_cost_context("my-workflow") as cost_context:
    # Your MLflow operations
    pass

summary = cost_context.get_summary()
print(f"Total cost: ${summary.total_cost:.6f}")
print(f"Cost by experiment: {summary.cost_by_experiment}")
print(f"Cost by team: {summary.cost_by_team}")
```

### Choosing the Right Cost Retrieval Method

GenOps provides three approaches for retrieving cost data. Choose based on your use case:

| Method | When to Use | Returns |
|--------|-------------|---------|
| Direct properties (`adapter.daily_usage`) | Quick summary of total costs | Float (total cost) and int (operation count) |
| `adapter.cost_aggregator.get_summary()` | Detailed breakdown by run/experiment | `MLflowCostSummary` with hierarchical costs |
| `create_mlflow_cost_context()` | Scoped cost tracking for specific operations | Context manager with isolated cost tracking |

**Example Decision Flow:**
- Need overall daily cost? â†’ Use `adapter.daily_usage`
- Need per-experiment breakdown? â†’ Use `adapter.cost_aggregator.get_summary()`
- Tracking specific workflow costs? â†’ Use `create_mlflow_cost_context()`

**Quick Reference:**
```python
# Approach 1: Simple total (fastest)
print(f"Total cost: ${adapter.daily_usage:.6f}")

# Approach 2: Detailed breakdown (most common)
summary = adapter.cost_aggregator.get_summary()
print(f"Cost by experiment: {summary.cost_by_experiment}")

# Approach 3: Scoped tracking (for specific workflows)
with create_mlflow_cost_context("workflow-name") as ctx:
    # Your operations here
    pass
print(f"Workflow cost: ${ctx.get_summary().total_cost:.6f}")
```

### Cost Attribution

Costs are automatically attributed across multiple dimensions:

```python
summary = adapter.cost_aggregator.get_summary()

# By experiment
for exp_name, cost in summary.cost_by_experiment.items():
    print(f"{exp_name}: ${cost:.6f}")

# By team
for team, cost in summary.cost_by_team.items():
    print(f"{team}: ${cost:.6f}")

# By project
for project, cost in summary.cost_by_project.items():
    print(f"{project}: ${cost:.6f}")

# By customer (if tracking multi-tenant)
for customer, cost in summary.cost_by_customer.items():
    print(f"{customer}: ${cost:.6f}")
```

---

## Governance Attributes

### Standard Attributes

All GenOps providers support these standard governance attributes:

| Attribute | Required | Description | Example |
|-----------|----------|-------------|---------|
| `team` | Yes | Team attribution | "ml-team" |
| `project` | Yes | Project tracking | "model-optimization" |
| `customer_id` | No | Customer attribution | "customer-123" |
| `environment` | No | Environment segregation | "production" |
| `cost_center` | No | Financial reporting | "ml-research" |

### MLflow-Specific Attributes

Additional attributes specific to MLflow:

| Attribute | Type | Description |
|-----------|------|-------------|
| `experiment_id` | Auto | MLflow experiment ID |
| `experiment_name` | Auto | MLflow experiment name |
| `run_id` | Auto | MLflow run ID |
| `run_name` | Auto | MLflow run name |
| `parent_run_id` | Optional | Parent run for hierarchy |
| `model_name` | Auto | Registered model name |
| `model_version` | Auto | Model version number |
| `model_stage` | Auto | Model lifecycle stage |
| `artifact_uri` | Auto | Artifact storage location |
| `ml_framework` | Auto | ML framework used (sklearn, pytorch, etc.) |

### Setting Governance Attributes

**1. At Adapter Level (applies to all operations):**
```python
adapter = instrument_mlflow(
    team="ml-team",
    project="model-training",
    customer_id="customer-123"
)
```

**2. At Run Level (overrides adapter):**
```python
with adapter.track_mlflow_run(
    experiment_name="exp",
    run_name="run",
    customer_id="customer-456",  # Override for this run
    cost_center="special-project"
) as run:
    pass
```

**3. Via Environment Variables:**
```bash
export GENOPS_TEAM="ml-team"
export GENOPS_PROJECT="model-training"
export GENOPS_CUSTOMER_ID="customer-123"
export GENOPS_ENVIRONMENT="production"
export GENOPS_COST_CENTER="ml-research"
```

### Viewing Governance Attributes

**In MLflow UI:**
All governance attributes appear as tags with `genops.` prefix:
- `genops.team`
- `genops.project`
- `genops.customer_id`
- `genops.environment`
- `genops.cost_center`

**Via API:**
```python
import mlflow

run = mlflow.get_run(run_id)
team = run.data.tags.get("genops.team")
project = run.data.tags.get("genops.project")
```

---

## Advanced Features

### 1. Multi-Provider Cost Tracking

Track costs across MLflow and other AI providers:

```python
from genops.providers.mlflow import instrument_mlflow
from genops.providers.openai import instrument_openai

# Initialize both providers
mlflow_adapter = instrument_mlflow(team="ml-team", project="training")
openai_adapter = instrument_openai(team="ml-team", project="training")

# Track combined workflow
with mlflow_adapter.track_mlflow_run(experiment_name="exp", run_name="run"):
    # MLflow operations tracked
    mlflow.log_param("param", "value")

    # OpenAI operations also tracked
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": "Generate features"}]
    )

    # Costs from both providers attributed to same team/project
```

### 2. Custom Cost Calculators

Override default cost calculations:

```python
from genops.providers.mlflow import MLflowCostCalculator

class CustomCostCalculator(MLflowCostCalculator):
    def __init__(self):
        super().__init__()
        # Override pricing
        self.pricing = {
            'tracking_api_call': 0.0002,  # Custom rate
            'storage': {
                's3': 0.030,  # Custom S3 rate
            }
        }

# Use custom calculator
adapter = instrument_mlflow(
    team="ml-team",
    cost_calculator=CustomCostCalculator()
)
```

### 3. Budget Enforcement

Set budget limits with alerts:

```python
adapter = instrument_mlflow(
    team="ml-team",
    budget_daily_limit=10.00  # $10 daily limit
)

# Operations tracked against budget
with adapter.track_mlflow_run(experiment_name="exp", run_name="run"):
    # If budget exceeded, warning raised
    mlflow.log_param("param", "value")

# Check budget status
if adapter.is_over_budget():
    print("âš ï¸  Daily budget exceeded!")
```

### 4. Policy Enforcement

Enforce governance policies:

```python
adapter = instrument_mlflow(
    team="ml-team",
    policies={
        'require_tags': ['owner', 'ticket'],
        'allowed_environments': ['dev', 'staging', 'prod'],
        'max_artifact_size_mb': 1000
    }
)

# Policy violations raise errors
with adapter.track_mlflow_run(experiment_name="exp", run_name="run"):
    mlflow.set_tag("owner", "engineer@company.com")  # Required
    mlflow.set_tag("ticket", "JIRA-123")            # Required
```

### 5. Auto-Logging Integration

Track auto-logged operations:

```python
import mlflow.sklearn

# Enable MLflow auto-logging
mlflow.sklearn.autolog()

# GenOps tracks all auto-logged operations
with adapter.track_mlflow_run(experiment_name="auto-exp", run_name="auto-run"):
    # Train model - parameters, metrics, model automatically logged
    model.fit(X_train, y_train)

# All auto-logged operations have governance telemetry
```

---

## API Reference

### GenOpsMLflowAdapter

Main adapter class for MLflow governance.

#### Constructor

```python
GenOpsMLflowAdapter(
    tracking_uri: Optional[str] = None,
    registry_uri: Optional[str] = None,
    team: Optional[str] = None,
    project: Optional[str] = None,
    customer_id: Optional[str] = None,
    environment: Optional[str] = None,
    cost_center: Optional[str] = None,
    **kwargs
)
```

**Parameters:**
- `tracking_uri`: MLflow tracking server URI (default: `MLFLOW_TRACKING_URI` env var)
- `registry_uri`: Model registry URI (default: `MLFLOW_REGISTRY_URI` env var)
- `team`: Team attribution (default: `GENOPS_TEAM` env var)
- `project`: Project tracking (default: `GENOPS_PROJECT` env var)
- `customer_id`: Customer attribution (default: `GENOPS_CUSTOMER_ID` env var)
- `environment`: Environment name (default: `GENOPS_ENVIRONMENT` env var)
- `cost_center`: Cost center code (default: `GENOPS_COST_CENTER` env var)

#### Methods

**`instrument_framework()`**

Enable governance instrumentation.

```python
adapter.instrument_framework()
```

**`uninstrument_framework()`**

Disable governance instrumentation and restore original MLflow methods.

```python
adapter.uninstrument_framework()
```

**`track_mlflow_run(experiment_name, run_name, **governance_attrs)`**

Context manager for tracking MLflow run lifecycle.

```python
with adapter.track_mlflow_run(
    experiment_name="my-experiment",
    run_name="my-run",
    customer_id="override-customer"
) as run:
    # Your MLflow operations
    pass
```

**Parameters:**
- `experiment_name`: MLflow experiment name
- `run_name`: MLflow run name
- `**governance_attrs`: Override governance attributes for this run

**Returns:** MLflow ActiveRun object

**`get_metrics()`**

Get current governance metrics.

```python
metrics = adapter.get_metrics()
```

**Returns:** Dictionary with keys:
- `daily_usage`: Total cost for today (float)
- `operation_count`: Number of operations tracked (int)
- `run_count`: Number of runs tracked (int)

**`calculate_cost(operation_context)`**

Calculate cost for an operation.

```python
cost = adapter.calculate_cost({
    'operation_type': 'log_artifact',
    'artifact_size_mb': 10.0,
    'storage_backend': 's3'
})
```

**Parameters:**
- `operation_context`: Dictionary with operation details

**Returns:** Cost in dollars (float)

### Factory Functions

**`instrument_mlflow(**kwargs)`**

Create and return configured MLflow adapter.

```python
from genops.providers.mlflow import instrument_mlflow

adapter = instrument_mlflow(
    tracking_uri="http://localhost:5000",
    team="ml-team",
    project="model-training"
)
```

**`auto_instrument_mlflow()`**

Enable zero-code auto-instrumentation with environment-based configuration.

```python
from genops.providers.mlflow import auto_instrument_mlflow

# Auto-detects configuration from environment variables
adapter = auto_instrument_mlflow()
```

**Returns:** Configured and instrumented MLflow adapter

### Validation Functions

**`validate_setup(**kwargs)`**

Comprehensive validation of MLflow + GenOps setup.

```python
from genops.providers.mlflow import validate_setup

result = validate_setup(
    tracking_uri="http://localhost:5000",
    check_connectivity=True,
    check_governance=True
)
```

**Parameters:**
- `tracking_uri`: MLflow tracking URI to validate
- `check_connectivity`: Test connection to MLflow server (default: True)
- `check_governance`: Validate governance features (default: True)

**Returns:** `ValidationResult` object with:
- `passed`: Overall validation status (bool)
- `issues`: List of `ValidationIssue` objects
- `dependencies`: Dependency check results (dict)
- `configuration`: Configuration values (dict)
- `connectivity`: Connectivity test results (dict)

**`print_validation_result(result)`**

Print formatted validation results.

```python
from genops.providers.mlflow import print_validation_result

print_validation_result(result)
```

### Cost Aggregator

**`create_mlflow_cost_context(context_name, **kwargs)`**

Context manager for cost tracking.

```python
from genops.providers.mlflow import create_mlflow_cost_context

with create_mlflow_cost_context("my-workflow") as cost_context:
    # Your MLflow operations
    pass

summary = cost_context.get_summary()
```

**Parameters:**
- `context_name`: Identifier for this cost context
- `**kwargs`: Additional configuration

**Returns:** `MLflowCostAggregator` instance

---

## Examples

### Basic Tracking

```python
from genops.providers.mlflow import instrument_mlflow
import mlflow

adapter = instrument_mlflow(
    tracking_uri="file:///tmp/mlruns",
    team="ml-team",
    project="basic-tracking"
)

with adapter.track_mlflow_run(
    experiment_name="basic-experiment",
    run_name="run-001"
) as run:
    # Log parameters
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("batch_size", 32)

    # Log metrics
    mlflow.log_metric("train_loss", 0.45)
    mlflow.log_metric("val_loss", 0.52)

    # Log artifact
    with open("summary.txt", "w") as f:
        f.write("Training summary")
    mlflow.log_artifact("summary.txt")

# View costs
metrics = adapter.get_metrics()
print(f"Total cost: ${metrics['daily_usage']:.6f}")
```

### Model Registry

```python
from genops.providers.mlflow import instrument_mlflow
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier

adapter = instrument_mlflow(
    tracking_uri="http://localhost:5000",
    registry_uri="http://localhost:5000",
    team="ml-team",
    project="model-registry"
)

with adapter.track_mlflow_run(
    experiment_name="model-training",
    run_name="rf-classifier"
) as run:
    # Train model
    model = RandomForestClassifier(n_estimators=100)
    model.fit(X_train, y_train)

    # Log model
    mlflow.sklearn.log_model(model, "model")

    # Register model
    model_uri = f"runs:/{run.info.run_id}/model"
    mlflow.register_model(model_uri, "rf-classifier")

print(f"Model registered with governance tracking")
```

### Hierarchical Runs

```python
from genops.providers.mlflow import instrument_mlflow
import mlflow

adapter = instrument_mlflow(
    team="ml-team",
    project="hyperparameter-tuning"
)

# Parent run for hyperparameter search
with adapter.track_mlflow_run(
    experiment_name="hp-search",
    run_name="search-parent"
) as parent:
    # Try different hyperparameters
    for lr in [0.001, 0.01, 0.1]:
        with adapter.track_mlflow_run(
            experiment_name="hp-search",
            run_name=f"lr-{lr}",
            parent_run_id=parent.info.run_id
        ) as child:
            mlflow.log_param("learning_rate", lr)
            # Train and evaluate...
            accuracy = train_model(lr)
            mlflow.log_metric("accuracy", accuracy)

# Parent run cost includes all children
```

### Multi-Tenant Tracking

```python
from genops.providers.mlflow import instrument_mlflow
import mlflow

adapter = instrument_mlflow(
    team="ml-platform",
    project="inference-service"
)

# Track per-customer usage
for customer in customers:
    with adapter.track_mlflow_run(
        experiment_name="inference",
        run_name=f"customer-{customer.id}",
        customer_id=customer.id  # Customer-level attribution
    ) as run:
        result = run_inference(customer.data)
        mlflow.log_metric("latency_ms", result.latency)
        mlflow.log_metric("tokens_used", result.tokens)

# Get per-customer costs
summary = adapter.cost_aggregator.get_summary()
for customer_id, cost in summary.cost_by_customer.items():
    print(f"{customer_id}: ${cost:.6f}")
```

---

## Troubleshooting

### Common Issues

#### MLflow not found

**Symptom:**
```
ImportError: No module named 'mlflow'
```

**Solution:**
```bash
pip install mlflow
```

#### Connection refused

**Symptom:**
```
ConnectionError: Cannot connect to MLflow tracking server
```

**Solutions:**
```bash
# Option 1: Start local MLflow server
mlflow ui --backend-store-uri file:///tmp/mlruns

# Option 2: Use file-based tracking
export MLFLOW_TRACKING_URI="file:///tmp/mlruns"

# Option 3: Check tracking URI
python -c "import mlflow; print(mlflow.get_tracking_uri())"
```

#### Governance attributes not set

**Symptom:**
```
WARNING: Governance attributes not configured
```

**Solution:**
```bash
export GENOPS_TEAM="your-team"
export GENOPS_PROJECT="your-project"
```

#### OpenTelemetry traces not exported

**Symptom:**
Traces not appearing in observability platform

**Solution:**
```bash
# Set OTLP endpoint
export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318"

# Or use specific backend
export OTEL_EXPORTER_OTLP_HEADERS="api-key=YOUR_API_KEY"

# Verify exporter configuration
python -c "from opentelemetry import trace; print(trace.get_tracer_provider())"
```

#### Cost calculations incorrect

**Symptom:**
Costs don't match expected values

**Solutions:**
1. Verify storage backend configuration
2. Check artifact sizes
3. Review cost calculator pricing
4. Enable debug logging

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Validation Script

Use the validation script for comprehensive diagnostics:

```bash
python examples/mlflow/setup_validation.py
```

Checks:
- âœ… Dependencies installed
- âœ… Configuration valid
- âœ… Connectivity working
- âœ… Governance features enabled

### Debug Mode

Enable detailed logging:

```python
import logging
from genops.providers.mlflow import instrument_mlflow

# Enable debug logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

adapter = instrument_mlflow(team="ml-team")
# Detailed logs will be printed
```

---

## Performance Considerations

### Instrumentation Overhead

GenOps adds minimal overhead to MLflow operations:

| Operation | Without GenOps | With GenOps | Overhead |
|-----------|----------------|-------------|----------|
| log_param | 1ms | 1.2ms | +20% |
| log_metric | 1ms | 1.2ms | +20% |
| log_artifact | 100ms | 102ms | +2% |
| log_model | 500ms | 505ms | +1% |

**Overhead sources:**
- Governance attribute extraction
- Cost calculation
- OpenTelemetry span creation
- Telemetry export (async)

### Optimization Strategies

**1. Disable instrumentation for performance-critical sections:**
```python
adapter.uninstrument_framework()
# High-performance operations here
adapter.instrument_framework()
```

**2. Use sampling for high-volume operations:**
```python
adapter = instrument_mlflow(
    team="ml-team",
    sampling_rate=0.1  # Sample 10% of operations
)
```

**3. Batch operations:**
```python
# Instead of many small log_metric calls
for i in range(1000):
    mlflow.log_metric(f"metric_{i}", value)

# Use batch operations when available
mlflow.log_metrics({f"metric_{i}": value for i in range(1000)})
```

**4. Async telemetry export:**
```python
adapter = instrument_mlflow(
    team="ml-team",
    async_export=True  # Export telemetry asynchronously
)
```

### Scaling Considerations

**High-Volume Scenarios:**
- 1000+ experiments: Consider aggregator instance per team
- 10,000+ runs/day: Enable sampling and batch processing
- Multiple teams: Use separate adapter instances with team isolation

**Storage Optimization:**
- Use local storage for development
- S3/Azure/GCS for production with lifecycle policies
- Archive old experiments to reduce costs

---

## Best Practices

### 1. Always Set Governance Attributes

```python
# âœ… Good
adapter = instrument_mlflow(
    team="ml-team",
    project="model-training"
)

# âŒ Bad
adapter = instrument_mlflow()  # Missing attribution
```

### 2. Use Context Managers

```python
# âœ… Good
with adapter.track_mlflow_run(experiment_name="exp", run_name="run"):
    mlflow.log_param("param", "value")
# Automatic cleanup and finalization

# âŒ Bad
mlflow.start_run()
mlflow.log_param("param", "value")
mlflow.end_run()
# Manual cleanup, costs may not be tracked
```

### 3. Validate Setup in CI/CD

```yaml
# .github/workflows/ci.yml
- name: Validate GenOps MLflow Setup
  run: python examples/mlflow/setup_validation.py
```

### 4. Monitor Costs Regularly

```python
# Check costs at end of workflow
metrics = adapter.get_metrics()
if metrics['daily_usage'] > budget:
    send_alert(f"Budget exceeded: ${metrics['daily_usage']}")
```

### 5. Use Environment-Specific Configuration

```python
# config/development.py
MLFLOW_TRACKING_URI = "file:///tmp/mlruns"
GENOPS_ENVIRONMENT = "development"

# config/production.py
MLFLOW_TRACKING_URI = "https://mlflow.company.com"
GENOPS_ENVIRONMENT = "production"
```

### 6. Document Governance Policies

```python
# Document in code
adapter = instrument_mlflow(
    team="ml-team",
    project="model-training",
    # POLICY: All production runs must have customer_id
    # POLICY: Maximum artifact size is 1GB
    # POLICY: Budget limit is $100/day
)
```

### 7. Clean Up After Tests

```python
import pytest

@pytest.fixture
def mlflow_adapter():
    adapter = instrument_mlflow(team="test-team")
    adapter.instrument_framework()
    yield adapter
    adapter.uninstrument_framework()  # Clean up
```

### 8. Use Typed Configuration

```python
from dataclasses import dataclass

@dataclass
class MLflowConfig:
    tracking_uri: str
    team: str
    project: str
    environment: str

config = MLflowConfig(
    tracking_uri="http://localhost:5000",
    team="ml-team",
    project="training",
    environment="production"
)

adapter = instrument_mlflow(**asdict(config))
```

---

## Additional Resources

- **MLflow Documentation**: https://mlflow.org/docs/latest/
- **GenOps GitHub**: https://github.com/KoshiHQ/GenOps-AI
- **OpenTelemetry**: https://opentelemetry.io
- **Examples**: `examples/mlflow/` directory
- **5-Minute Quickstart**: `docs/mlflow-quickstart.md`

## Support

- **GitHub Issues**: https://github.com/KoshiHQ/GenOps-AI/issues
- **Documentation**: https://github.com/KoshiHQ/GenOps-AI/tree/main/docs
- **Community**: Join our discussions on GitHub

---

**Last Updated**: 2026-01-11
**Version**: 0.1.0
**Status**: Production Ready
